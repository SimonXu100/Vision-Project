{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 - Implemenet NN Layers\n",
    "-----\n",
    "\n",
    "### 1.1 Fully Connected Layer\n",
    "Before we get started, let's recall what happens in the forward pass of a full-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Linear():\n",
    "    \"\"\"A fully-connected NN layer.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_units: int\n",
    "        The number of neurons in the layer.\n",
    "    input_shape: tuple\n",
    "        The expected input shape of the layer. For dense layers a single digit specifying\n",
    "        the number of features of the input. Must be specified if it is the first layer in\n",
    "        the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_units, input_shape=None):\n",
    "        # For simplisity, we omit optimizer in our homework.\n",
    "        # Therefore, you do not need to worry about parameter update.\n",
    "        self.layer_input = None\n",
    "        self.input_shape = input_shape\n",
    "        self.n_units = n_units\n",
    "        self.trainable = True\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        # Initialize the weights\n",
    "        limit = 1 / math.sqrt(self.input_shape[0])\n",
    "        self.W  = np.random.uniform(-limit, limit, (self.input_shape[1], self.n_units))\n",
    "        self.b = np.zeros((1, self.n_units))\n",
    "\n",
    "    def forward_pass(self, inp):\n",
    "        self.layer_input = inp\n",
    "        return np.dot(inp, self.W) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provided some helper functions that might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SE(out, target):\n",
    "    '''\n",
    "    return square error.\n",
    "    '''\n",
    "    return 0.5 * (target - out)**2\n",
    "\n",
    "def get_target(inp, W, b):\n",
    "    '''\n",
    "    W and b are assumed ideal weights and bias.\n",
    "    '''\n",
    "    return np.dot(inp, W) + b\n",
    "\n",
    "def grad_check(layer, inp, W, b):\n",
    "    '''\n",
    "    calculate gradient from numerical method, we compare the analytical gradient and numerical gradient.\n",
    "    \n",
    "    We say your calculated gradients are correct when the mean square error between \n",
    "    standard gradient and your gradient is below some threshold.\n",
    "    \n",
    "    return true when gradients of W, b and inp are calculated correctly.\n",
    "    '''\n",
    "    res = True\n",
    "    target = get_target(inp, W, b)\n",
    "    out = layer.forward_pass(inp)\n",
    "    y = SE(target, out)\n",
    "    loss = target - out\n",
    "    accum_grad = layer.backward_pass(loss)\n",
    "    \n",
    "    W_shape = layer.W.shape\n",
    "    b_shape = layer.b.shape\n",
    "    inp_shape = inp.shape\n",
    "    \n",
    "    limit = 1e-6\n",
    "    threshold = 1e-8 * inp_shape[0]**2\n",
    "    \n",
    "    W_diff = np.zeros(W_shape)\n",
    "    for i in range(W_shape[0]):\n",
    "        noise = np.random.rand(W_shape[1]) * limit\n",
    "        layer.W[i,:] += noise\n",
    "        out2 = layer.forward_pass(inp)\n",
    "        y2 = SE(target, out2)\n",
    "        W_diff[i,:] = np.sum(y - y2, axis=0) / noise\n",
    "        layer.W[i,:] -= noise\n",
    "        \n",
    "    res &= (np.sum((W_diff - layer.grad_W)**2) < threshold)\n",
    "    \n",
    "    noise = np.random.rand(*b_shape) * limit\n",
    "    layer.b += noise\n",
    "    out2 = layer.forward_pass(inp)\n",
    "    y2 = SE(target, out2)\n",
    "    b_diff = np.sum(y - y2, axis=0) / noise\n",
    "    layer.b -= noise   \n",
    "    \n",
    "    res &= (np.sum((b_diff - layer.grad_b)**2) < threshold)\n",
    "    \n",
    "    inp_diff = np.zeros(inp_shape)\n",
    "    for j in range(inp_shape[1]):\n",
    "        noise = np.random.rand(inp_shape[0]) * limit\n",
    "        inp[:,j] += noise\n",
    "        out2 = layer.forward_pass(inp)\n",
    "        y2 = SE(target, out2)\n",
    "        inp_diff[:,j] = np.sum(y - y2, axis=1) / noise\n",
    "        inp[:,j] -= noise\n",
    " \n",
    "    res &= (np.sum((inp_diff - accum_grad)**2) < threshold) \n",
    "    \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Backward Pass\n",
    "Now you can start building your own backward function of the fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass_fc(self, accum_grad):\n",
    "    '''\n",
    "    TODO: Implement the backward_pass_fc here.\n",
    "    \n",
    "    Parameter:\n",
    "    ----------\n",
    "        accum_grad: gradient propogated back from the next layer\n",
    "        \n",
    "    Return:\n",
    "    ----------\n",
    "        accum_grad_result: gradient propogated back from the this layer\n",
    "    '''\n",
    "    \n",
    "    self.grad_W = 0\n",
    "    self.grad_b = 0\n",
    "    accum_grad_result = np.zeros(self.layer_input.shape)\n",
    "    \n",
    "    # the gradient of weights\n",
    "    self.grad_W = self.layer_input.T.dot(accum_grad)\n",
    "    \n",
    "    # the gradient of bias\n",
    "    grad_out_b = np.ones([1,100])  \n",
    "    self.grad_b = grad_out_b.dot(accum_grad)\n",
    "      \n",
    "    # the gradient of input\n",
    "    accum_grad_result = accum_grad.dot(self.W.T) \n",
    "    \n",
    "    \n",
    "    return accum_grad_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your implementation\n",
    "Use grad_check to test the correctness of your backward implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Testing Backward Pass: Pass!\n"
     ]
    }
   ],
   "source": [
    "Linear.backward_pass = backward_pass_fc\n",
    "\n",
    "inp = np.random.rand(100,3)\n",
    "layer = Linear(2, inp.shape)\n",
    "\n",
    "W = np.random.rand(3,2)\n",
    "b = np.random.rand(1,2)\n",
    "\n",
    "if grad_check(layer, inp, W, b):\n",
    "    print(\"[INFO] Testing Backward Pass: Pass!\")\n",
    "else:\n",
    "    print(\"[WARN] Testing Backward Pass: Fail!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convolutional Layer\n",
    "Before we get started, let's recall what happens in the forward pass of a convolutional layer.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D():\n",
    "    \"\"\"A 2D Convolution Layer.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_filters: int\n",
    "        The number of filters that will convolve over the input matrix. The number of channels\n",
    "        of the output shape.\n",
    "    filter_shape: tuple\n",
    "        A tuple (filter_height, filter_width).\n",
    "    input_shape: tuple\n",
    "        The shape of the expected input of the layer. (batch_size, channels, height, width)\n",
    "        Only needs to be specified for first layer in the network.\n",
    "    padding: string\n",
    "        Either 'same' or 'valid'. 'same' results in padding being added so that the output height and width\n",
    "        matches the input height and width. For 'valid' no padding is added.\n",
    "        By default, we use 'same' to test the implementation.\n",
    "    stride: int\n",
    "        The stride length of the filters during the convolution over the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_filters, filter_shape, input_shape, padding='same', stride=1):\n",
    "        self.n_filters = n_filters\n",
    "        self.filter_shape = filter_shape\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.input_shape = input_shape\n",
    "        self.trainable = True\n",
    "        self.W = None\n",
    "        self.w0 = None\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        # Initialize the weights\n",
    "        filter_height, filter_width = self.filter_shape\n",
    "        batch, channels, height, width = self.input_shape\n",
    "        limit = 1 / math.sqrt(np.prod(self.filter_shape))\n",
    "        self.W  = np.random.uniform(-limit, limit, size=(self.n_filters, channels, filter_height, filter_width))\n",
    "        self.w0 = np.zeros((self.n_filters, 1))\n",
    "\n",
    "    def output_shape(self):\n",
    "        batch, channels, height, width = self.input_shape\n",
    "        pad_h, pad_w = determine_padding(self.filter_shape, output_shape=self.padding)\n",
    "        output_height = (height + np.sum(pad_h) - self.filter_shape[0]) / self.stride + 1\n",
    "        output_width = (width + np.sum(pad_w) - self.filter_shape[1]) / self.stride + 1\n",
    "        return self.n_filters, int(output_height), int(output_width)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        batch_size, channels, height, width = X.shape\n",
    "        self.layer_input = X\n",
    "        # Turn image shape into column shape\n",
    "        # (enables dot product between input and weights)\n",
    "        self.X_col = image_to_column(X, self.filter_shape, stride=self.stride, output_shape=self.padding)\n",
    "        print(\"the shape of self.X_col  \", np.shape(self.X_col))\n",
    "        #the shape of self.X_col   (27, 25)\n",
    "        # Turn weights into column shape\n",
    "        self.W_col = self.W.reshape((self.n_filters, -1))\n",
    "        \n",
    "        print(\"the shape of self.W.col   \", np.shape(self.W_col))\n",
    "        #the shape of self.W.col    (5, 27)\n",
    "        # Calculate output\n",
    "        output = self.W_col.dot(self.X_col) + self.w0\n",
    "        \n",
    "        print(\"the shape of output_forward1:  \", np.shape(output))\n",
    "        # the shape of output_forward1:   (5, 25)\n",
    "        # Reshape into (n_filters, out_height, out_width, batch_size)\n",
    "        output = output.reshape(self.output_shape() + (batch_size, ))\n",
    "        print(\"the shape of output_forward2:  \", np.shape(output))\n",
    "        # the shape of output_forward2:   (5, 5, 5, 1)\n",
    "        # Redistribute axises so that batch size comes first\n",
    "        print(\"the shape of output_forward_final:  \", np.shape(output.transpose(3,0,1,2)))\n",
    "        #\n",
    "        return output.transpose(3,0,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provided some helper functions that might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method which turns the image shaped input to column shape.\n",
    "# Used during the forward pass.\n",
    "# Reference: CS231n Stanford\n",
    "def image_to_column(images, filter_shape, stride, output_shape='same'):\n",
    "    filter_height, filter_width = filter_shape\n",
    "\n",
    "    pad_h, pad_w = determine_padding(filter_shape, output_shape)\n",
    "    print(\"the pad_h:  \", pad_h )\n",
    "    print(\"teh pad_w   \", pad_w)\n",
    "    \n",
    "\n",
    "    # Add padding to the image\n",
    "    images_padded = np.pad(images, ((0, 0), (0, 0), pad_h, pad_w), mode='constant')\n",
    "    \n",
    "    print(\"the shape of images_padded  \", np.shape(images_padded))\n",
    "    \n",
    "    # Calculate the indices where the dot products are to be applied between weights\n",
    "    # and the image\n",
    "    k, i, j = get_im2col_indices(images.shape, filter_shape, (pad_h, pad_w), stride)\n",
    "    \n",
    "    print(\"K:  \", np.shape(k))\n",
    "    print(\"i   \", np.shape(i))\n",
    "    print(\"j   \", np.shape(j))\n",
    "    #print(\"K   \", k)\n",
    "    #print(\"i  \", i)\n",
    "    #print(\"j  \", j)\n",
    "    \n",
    "    # Get content from image at those indices\n",
    "    cols = images_padded[:, k, i, j]\n",
    "    channels = images.shape[1]\n",
    "    print(\"cols_before_reshape:  \", np.shape(cols))\n",
    "    # Reshape content into column shape\n",
    "    \n",
    "    cols = cols.transpose(1, 2, 0).reshape(filter_height * filter_width * channels, -1)\n",
    "    return cols\n",
    "\n",
    "# Reference: CS231n Stanford\n",
    "def get_im2col_indices(images_shape, filter_shape, padding, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    batch_size, channels, height, width = images_shape\n",
    "    filter_height, filter_width = filter_shape\n",
    "    pad_h, pad_w = padding\n",
    "    out_height = int((height + np.sum(pad_h) - filter_height) / stride + 1)\n",
    "    out_width = int((width + np.sum(pad_w) - filter_width) / stride + 1)\n",
    "\n",
    "    i0 = np.repeat(np.arange(filter_height), filter_width)\n",
    "    i0 = np.tile(i0, channels)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(filter_width), filter_height * channels)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(channels), filter_height * filter_width).reshape(-1, 1)\n",
    "\n",
    "    return (k, i, j)\n",
    "\n",
    "# Method which calculates the padding based on the specified output shape and the\n",
    "# shape of the filters\n",
    "def determine_padding(filter_shape, output_shape=\"same\"):\n",
    "    # No padding\n",
    "    if output_shape == \"valid\":\n",
    "        return (0, 0), (0, 0)\n",
    "    # Pad so that the output shape is the same as input shape (given that stride=1)\n",
    "    elif output_shape == \"same\":\n",
    "        filter_height, filter_width = filter_shape\n",
    "\n",
    "        # Derived from:\n",
    "        # output_height = (height + pad_h - filter_height) / stride + 1\n",
    "        # In this case output_height = height and stride = 1. This gives the\n",
    "        # expression for the padding below.\n",
    "        pad_h1 = int(math.floor((filter_height - 1)/2))\n",
    "        pad_h2 = int(math.ceil((filter_height - 1)/2))\n",
    "        pad_w1 = int(math.floor((filter_width - 1)/2))\n",
    "        pad_w2 = int(math.ceil((filter_width - 1)/2))\n",
    "\n",
    "        return (pad_h1, pad_h2), (pad_w1, pad_w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Backward Pass\n",
    "Now you can start building your own backward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass_conv(layer, accum_grad):\n",
    "    '''\n",
    "    TODO: Implement the backward_pass_fc here.\n",
    "    \n",
    "    Parameter:\n",
    "    ----------\n",
    "        accum_grad: gradient propogated back from the next layer\n",
    "        \n",
    "    Return:\n",
    "    ----------\n",
    "        accum_grad_result: gradient propogated back from the this layer\n",
    "    '''\n",
    "    accum_grad_result = np.zeros(layer.layer_input.shape)\n",
    "     \n",
    "        \n",
    "    # the gradient of input\n",
    "    # from 1*5*5*5 to 1*3*5*5 \n",
    "    # full convolution: 5*7*7*7\n",
    "    #accum_grad_result = convolve(accum_grad, layer.W, mode = 'full')\n",
    "    \n",
    "    # from 1*5*5*5 --- W: 5*3*3*3\n",
    "    # from 1*5*5*5 --- S: 3*5*3*3\n",
    "    #output.transpose(3,0,1,2))\n",
    "    back_pass_w = layer.W.transpose(1,0,2,3)\n",
    "    \n",
    "    \n",
    "    #test flip\n",
    "    back_pass_w = np.flip(back_pass_w, (2,3))\n",
    "    \n",
    "    \n",
    "    print(\"the shape of back_pass_w  \",np.shape(back_pass_w) )\n",
    "    # the shape of test_w: 3*5*3*3 \n",
    "   \n",
    "    \n",
    "    #initialize: \n",
    "    # batch * channels * height * weight = 1 * 3 * 5 * 5\n",
    "    #grad_input_shape = layer.layer_input.shape\n",
    "    \n",
    "    # filter_shape: layer.filter_shape\n",
    "    \n",
    "    # the num of filters: 3\n",
    "    n_filters = back_pass_w.shape[0]\n",
    "    \n",
    "    # padding: \"same\"\n",
    "    padding = \"same\"\n",
    "    # stride = 1\n",
    "    stride = layer.stride\n",
    "    \n",
    "    \n",
    "    #modify the forward_pass, consider the accum_grad as the input\n",
    "    batch_size, channels, height, width = accum_grad.shape\n",
    "    \n",
    "       \n",
    "    X_col_accum_grad = image_to_column(accum_grad, layer.filter_shape, stride=1, output_shape=\"same\")\n",
    "    \n",
    "    W_col_back_pass = back_pass_w.reshape((n_filters, -1))\n",
    "        \n",
    "    \n",
    "    result = W_col_back_pass.dot(X_col_accum_grad)\n",
    "    # determine the output_shape \n",
    "    pad_h, pad_w = determine_padding(layer.filter_shape, output_shape= padding)\n",
    "    output_height = (height + np.sum(pad_h) - layer.filter_shape[0]) / stride + 1\n",
    "    output_width = (width + np.sum(pad_w) - layer.filter_shape[1]) / stride + 1\n",
    "    \n",
    "    # Reshape into (n_filters, out_height, out_width, batch_size)\n",
    "    result_shape = (n_filters, int(output_height), int(output_width))\n",
    "    result = result.reshape(result_shape + (batch_size, ))\n",
    "    \n",
    "    # Redistribute axises so that batch size comes first\n",
    "    #print(\"the shape of output_forward_final:  \", np.shape(output.transpose(3,0,1,2)))\n",
    "    accum_grad_result = result.transpose(3,0,1,2)\n",
    "    print(\"the shape of accum_grad_result    \", np.shape(accum_grad_result))\n",
    "    \n",
    "\n",
    "    return accum_grad_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your implementation:\n",
    "We use preloaded input, output, weight and bias tensor to test the implementation of your forward pas and backward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_test():\n",
    "    Conv2D.backward_pass = backward_pass_conv\n",
    "    \n",
    "    # np.load return the k,v pair of the name and value of numpy matrix\n",
    "    data = np.load('test.npz')\n",
    "    \n",
    "    #print(\"the type of data  \",data)\n",
    "    \n",
    "    # read the input from npz file\n",
    "    input_tensor = data['input_tensor']\n",
    "    print(\"the type of input_tensor  \",np.shape(input_tensor))\n",
    "    \n",
    "    \n",
    "    # read the forward pass result from npz file\n",
    "    output_tensor = data['output_tensor']\n",
    "    print(\"the type of output_tensor  \",np.shape(output_tensor))\n",
    "    \n",
    "    # read the target from npz file\n",
    "    target_tensor = data['target_tensor']\n",
    "    print(\"the type of target_tensor  \",np.shape(target_tensor))\n",
    "    \n",
    "    \n",
    "    # read the backward pass result from npz file\n",
    "    accum_grad = data['accum_grad']\n",
    "    print(\"the type of accum_grad  \",np.shape(input_tensor))\n",
    "    \n",
    "    # read the preloaded weight and bias from npz file\n",
    "    w0 = data['w0']\n",
    "    print(\"the type of w0  \", np.shape(w0))\n",
    "    # \n",
    "    W = data['W']\n",
    "    print(\"the type of W  \", np.shape(W))\n",
    "    \n",
    "\n",
    "    # read the configuration from npz file\n",
    "    filter_size = data['filter_size']\n",
    "    filter_num = data['filter_num']\n",
    "    print(\"the type of filter_size  \", filter_size)\n",
    "    print(\"the type of filter_num  \", filter_num)\n",
    "    \n",
    "    \n",
    "    # configure the \n",
    "    layer = Conv2D(n_filters=filter_num, filter_shape=(filter_size, filter_size), input_shape=input_tensor.shape)\n",
    "    layer.W, layer.w0 = W, w0\n",
    "    predict_tensor = layer.forward_pass(input_tensor)\n",
    "\n",
    "    print(\"the type of predicted tensor  \", np.shape(predict_tensor))\n",
    "    \n",
    "    print(\"SE:  \",  np.shape( SE(predict_tensor, output_tensor))  )\n",
    "    \n",
    "    \n",
    "    # Test the forward pass implementation\n",
    "    if SE(predict_tensor, output_tensor).all() < 1e-1:\n",
    "        print(\"[INFO] Testing Forward: Pass!\")\n",
    "    else:\n",
    "        print(\"[WARN] Testing Forward: Fail!\")\n",
    "    \n",
    "    # use the tensors read from the npz file to compute the loss\n",
    "    loss = target_tensor - output_tensor\n",
    "    \n",
    "    print(\"the shape of loss  \", np.shape(loss))\n",
    "    \n",
    "    predict_accum_grad = layer.backward_pass(loss)\n",
    "    \n",
    "    print(\"the shape of SE   \", SE(predict_accum_grad, accum_grad) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Test the backward pass implementation\n",
    "    if SE(predict_accum_grad, accum_grad).all() < 1e-1:\n",
    "        print(\"[INFO] Testing Backward: Pass!\")\n",
    "    else:\n",
    "        print(\"[WARN] Testing Backward: Fail!\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of input_tensor   (1, 3, 5, 5)\n",
      "the type of output_tensor   (1, 5, 5, 5)\n",
      "the type of target_tensor   (1, 5, 5, 5)\n",
      "the type of accum_grad   (1, 3, 5, 5)\n",
      "the type of w0   (5, 1)\n",
      "the type of W   (5, 3, 3, 3)\n",
      "the type of filter_size   3\n",
      "the type of filter_num   5\n",
      "the pad_h:   (1, 1)\n",
      "teh pad_w    (1, 1)\n",
      "the shape of images_padded   (1, 3, 7, 7)\n",
      "K:   (27, 1)\n",
      "i    (27, 25)\n",
      "j    (27, 25)\n",
      "cols_before_reshape:   (1, 27, 25)\n",
      "the shape of self.X_col   (27, 25)\n",
      "the shape of self.W.col    (5, 27)\n",
      "the shape of output_forward1:   (5, 25)\n",
      "the shape of output_forward2:   (5, 5, 5, 1)\n",
      "the shape of output_forward_final:   (1, 5, 5, 5)\n",
      "the type of predicted tensor   (1, 5, 5, 5)\n",
      "SE:   (1, 5, 5, 5)\n",
      "[INFO] Testing Forward: Pass!\n",
      "the shape of loss   (1, 5, 5, 5)\n",
      "the shape of back_pass_w   (3, 5, 3, 3)\n",
      "the pad_h:   (1, 1)\n",
      "teh pad_w    (1, 1)\n",
      "the shape of images_padded   (1, 5, 7, 7)\n",
      "K:   (45, 1)\n",
      "i    (45, 25)\n",
      "j    (45, 25)\n",
      "cols_before_reshape:   (1, 45, 25)\n",
      "the shape of accum_grad_result     (1, 3, 5, 5)\n",
      "the shape of SE    [[[[2.86939544e-02 3.94257439e-03 2.73661346e-03 5.33234521e-02\n",
      "    3.18252487e-02]\n",
      "   [5.08775063e-03 9.68068035e-05 4.76686157e-03 2.45725667e-02\n",
      "    3.91714400e-04]\n",
      "   [1.53545843e-02 5.46242681e-03 5.31298934e-02 3.59281391e-02\n",
      "    3.77728930e-03]\n",
      "   [5.21756161e-04 1.53100721e-04 2.02183775e-02 1.03829194e-02\n",
      "    4.44499091e-02]\n",
      "   [8.58757535e-04 1.92995204e-04 4.36769362e-02 3.40365926e-02\n",
      "    1.62915091e-02]]\n",
      "\n",
      "  [[2.93380048e-02 4.50327294e-02 4.52941777e-02 1.66933061e-02\n",
      "    4.64516441e-02]\n",
      "   [1.01617776e-02 1.88955648e-02 1.15673175e-02 5.59538613e-03\n",
      "    4.28767657e-02]\n",
      "   [3.73992120e-03 1.35267161e-02 2.51605582e-02 2.32124485e-02\n",
      "    7.65673175e-03]\n",
      "   [4.33589708e-02 6.34427191e-03 4.38802384e-03 1.40838073e-03\n",
      "    5.08028061e-02]\n",
      "   [1.10735333e-03 1.01930700e-02 3.20251876e-03 3.36009764e-02\n",
      "    3.42397839e-02]]\n",
      "\n",
      "  [[1.01250389e-02 3.24199495e-02 1.20994601e-02 5.26781991e-02\n",
      "    2.92081944e-02]\n",
      "   [4.54381082e-02 2.95767974e-04 8.69987278e-03 2.11136559e-02\n",
      "    6.86304422e-03]\n",
      "   [1.62918267e-02 2.69315298e-02 6.62260960e-06 1.47305893e-02\n",
      "    7.14927485e-04]\n",
      "   [1.17632527e-02 3.62978788e-02 1.49295998e-02 1.80001470e-02\n",
      "    1.71862863e-03]\n",
      "   [7.12019072e-05 1.00974196e-28 0.00000000e+00 0.00000000e+00\n",
      "    0.00000000e+00]]]]\n",
      "[INFO] Testing Backward: Pass!\n"
     ]
    }
   ],
   "source": [
    "conv_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
